# -*- coding: utf-8 -*-
"""science.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13eC-pn1_Z2ojQcHb6wfHm_sZPtzMXjgX
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
# Add headers to avoid 403 error of website
HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36',}

def extract_journal_info(journal_url):
    response = requests.get(journal_url, headers = HEADERS)
    soup = BeautifulSoup(response.content, 'html.parser')

    # parse html to crawl data
    journal_name = soup.find("div", {"class": "page-container"}).find("div", {"class", "sc-iBPRYJ fxMpMn"}).find("span",{"class": "anchor-text"}).getText()

    impact_factor_craw = soup.find("span",{"class":"text-l u-display-block"})
    impact_factor = impact_factor_craw.getText() if impact_factor_craw else ""


    section_issn = soup.find("div",{"class":"js-issn text-s"}).find_all("div")
    ISSN = [link.getText() for link in section_issn]
    issn = ""
    online_issn = ""
    print_issn = ""
    linking_issn = ""

    if section_issn:
      for i in ISSN:
        pair_value = i.split(': ')
        if pair_value[0] == 'ISSN':
          issn = pair_value[1]
        if pair_value[0] == 'Online ISSN':
          online_issn = pair_value[1]
        if pair_value[0] == 'Print ISSN':
          print_issn = pair_value[1]
        if pair_value[0] == 'Linking ISSN':
          linking_issn = pair_value[1]
    # Save to file json
    j_son = {
        'Journal Name': journal_name,
        'Impact Factor (2 years)': impact_factor,
        'ISSN': issn,
        'Online ISSN': online_issn,
        'Print ISSN': print_issn,
        'Linking ISSN': linking_issn
    }
    print(j_son)
    return j_son

base_url_0 = "https://www.sciencedirect.com/browse/journals-and-books?page="
base_url_1 = "&contentType=JL"

journal_data = []

page_number = 1
while True:
  # access to webpage containing journals
  response = requests.get(f"{base_url_0}{page_number}{base_url_1}",headers = HEADERS)
  soup = BeautifulSoup(response.content, 'html.parser')
  journal_links = soup.find_all(class_='anchor js-publication-title anchor-default')
  journal_urls = [link.get('href') for link in journal_links]
  if len(journal_urls) == 0:
    continue
  # access to each journal
  for journal_url in journal_urls:
    journal_info = extract_journal_info("https://www.sciencedirect.com/" + journal_url)
    if journal_info:
      journal_data.append(journal_info)
  # set up stopping program when it is in last page
  next_page = soup.find("button", {"class" : "button-alternative button-alternative-secondary medium-alternative button-alternative-icon-right"})
  disabled = next_page.get('disabled')
  if disabled is None:
    page_number += 1
  else:
    break
df = pd.DataFrame(journal_data)
df.to_excel('sciencedirect.xlsx')